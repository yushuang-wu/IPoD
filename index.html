<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="IPoD">
  <meta name="keywords" content="3D Reconstruction, Point Diffusion, Implicit Field Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D Object Reconstruction from Single RGB-D Images</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://gaplab.cuhk.edu.cn">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D Object Reconstruction from Single RGB-D Images</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=x5gpN0sAAAAJ">Yushuang Wu</a><sup>1,2</sup>, </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Dt5LAqcAAAAJ">Luyue Shi</a><sup>1</sup>, </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=GeSCNR4AAAAJ">Junhao Cai</a><sup>4</sup>, </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=m3tqxRQAAAAJ">Weihao Yuan</a><sup>3</sup>, </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=YJFpZ2kAAAAJ">Lingteng Qiu</a><sup>1</sup>, </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=GHOQKCwAAAAJ">Zilong Dong</a><sup>3</sup>, </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=FJwtMf0AAAAJ">Liefeng Bo</a><sup>3</sup>, </span>
              <span class="author-block">
                <a href="https://sse.cuhk.edu.cn/en/faculty/cuishuguang">Shuguang Cui</a><sup>2,1</sup>, </span>
              <span class="author-block">
                <a href="https://gaplab.cuhk.edu.cn">Xiaoguang Han</a><sup>2,1,#</sup></span>
            </div>
            <h1 style="font-size:23px;font-weight:bold">CVPR 2024</h1>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>FNii,CUHKSZ,  </span>
              <span class="author-block"><sup>2</sup>SSE,CUHKSZ,  </span>
              <span class="author-block"><sup>3</sup>Alibaba Group,  </span>
              <span class="author-block"><sup>4</sup>HKUST</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/to_be_released.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/to_be_released" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/yushuang-wu/IPoD"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div align="center"> <img src="static/images/teaser.png" width="900px"> </div>
        <div class="content has-text-centered">
          Our work focuses on the task of generalizable 3D object
          reconstruction from a single RGB-D image. The proposed method
          conducts implicit field learning with point diffusion that iteratively
          denoises a point cloud as adaptive query points for better implicit
          field learning, which leads to high reconstruction quality on both
          the global shape and fine details.
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            Generalizable 3D object reconstruction from single-view
            RGB-D images remains a challenging task, particularly
            with real-world data. Current state-of-the-art methods 
            develop Transformer-based implicit field learning, necessitating 
            an intensive learning paradigm that requires dense
            query-supervision uniformly sampled throughout the entire
            space. We propose a novel approach, IPoD, which harmonizes 
            implicit field learning with point diffusion. This
            approach treats the query points for implicit field learning 
            as a noisy point cloud for iterative denoising, allowing 
            for their dynamic adaptation to the target object shape.
            Such adaptive query points harness diffusion learning’s capability 
            for coarse shape recovery and also enhances the implicit 
            representation’s ability to delineate finer details. 
            Besides, an additional self-conditioning mechanism is designed 
            to use implicit predictions as the guidance of diffusion 
            learning, leading to a cooperative system. Experiments 
            conducted on the CO3D-v2 dataset affirm the superiority of 
            IPoD, achieving 7.8% improvement in F-score and 28.6% in 
            Chamfer distance over existing methods. The generalizability
            of IPoD is also demonstrated on the MVImgNet dataset.
          </div>
        </div>
      </div>

    </div>
  </section>
  
 


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          
          <div class="column is-full-width">
            <!-- Dataset Exhibition -->
            <h2 class="title is-3">1. Dataset Exhibition of ScanSalon</h2>
            <div align="center"> <img src="static/images/dataset_vis.png" width="900px"> </div>
            <div class="content has-text-centered">
            Visualization of samples in the proposed ScanSalon dataset.
          </div>
          
          

          <div class="content has-text-centered">
            <video class="video-fluid w-100" controls autoplay loop muted>
              <source src="static/videos/real_scan.mp4" type="video/mp4"  width="750px" />
            </video>
            <p>Real Scans</p>
          </div>

          <div class="content has-text-centered">
            <video class="video-fluid w-100" controls autoplay loop muted>
              <source src="static/videos/mesh.mp4" type="video/mp4"  width="750px" />
            </video>
            <p>Created Meshes</p>
          </div>

        </div>
      </div>
    </div>
  </section>
    
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">

          <div class="column is-full-width">
            <!-- Dataset Exhibition -->
            <h2 class="title is-3">2. Method and Results</h2>
            <div align="center"> <img src="static/images/method.png" width="900px"> </div>
            <div class="content has-text-centered">
            Overview of the proposed method. Two IF-Net encoders are used for the source and the target domain, respectively, and they
            share an implicit function decoder. The cross-domain feature fusion (CDFF) works by adaptively combining the global-level and local-level
            knowledge learned from the source and target domain, respectively. The volume-consistency self-training (VCST) works by enforcing the
            prediction consistency between two different augmented views to learn the local details.
          </div>
            
            
          <div class="content has-text-centered">
            <div align="center"> <img src="static/images/results_vis2.png" width="900px"> </div>
            <p>Qualitative comparison between our method and IF-Net on shape completion with only 3% (left) and 5% labels (right) for training. </p>
          </div>
          
        </div>
      </div>
    </div>
  </section>
    
   
  
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          
          <div class="column is-full-width">
            <!-- Dataset Exhibition -->
            <h2 class="title is-3">3. ScanSalon Sample Visulization</h2>
          </div>

          <div class="content has-text-centered">
            <video  width="750px" class="video-fluid w-100" controls autoplay loop muted>
              <source src="static/videos/sofa.mp4" type="video/mp4" />
            </video>
            <p>Class: Sofa</p>
          </div>

          <div class="content has-text-centered">
            <video width="750px" class="video-fluid w-100" controls autoplay loop muted>
              <source src="static/videos/bed.mp4" type="video/mp4" />
            </video>
            <p>Class: bed</p>
          </div>

          <div class="content has-text-centered">
            <video width="750px" class="video-fluid w-100" controls autoplay loop muted>
              <source src="static/videos/chair.mp4" type="video/mp4" />
            </video>
            <p>Class: Chair</p>
          </div>

          <div class="content has-text-centered">
            <video width="750px" class="video-fluid w-100" controls autoplay loop muted>
              <source src="static/videos/desk.mp4" type="video/mp4" />
            </video>
            <p>Calss: Desk</p>
          </div>

          <div class="content has-text-centered">
            <video width="750px" class="video-fluid w-100" controls autoplay loop muted>
              <source src="static/videos/lamp.mp4" type="video/mp4" />
            </video>
            <p>Class: Lamp</p>
          </div>

          <div class="content has-text-centered">
            <video width="750px" class="video-fluid w-100" controls autoplay loop muted>
              <source src="static/videos/car.mp4" type="video/mp4" />
            </video>
            <p>Class: Car</p>
          </div>

        </div>
      </div>
    </div>
  </section>
    
    
  

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{wu2023ipod,
  title={IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D Object Reconstruction from Single RGB-D Images},
  author={Yushuang, Wu and Luyue, Shi and Junhao, Cai and Weihao, Yuan and Lingteng, Qiu and Zilong, Dong and Liefeng, Bo and Shuguang, Cui and Xiaoguang, Han},
  booktitle={The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR)},
  year={2024}}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div align="center" class="container">
      <div class="columns is-centered">
        <div class="content">
          This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
